{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test: (value: Int)Int\n",
       "res72: Int = 0\n"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(value: Int = 0): Int = value\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factorial: (n: Int)Int\n",
       "res29: Int = 24\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n: Int): Int = {\n",
    "   var countdown = n\n",
    "   var total = 1\n",
    "   while(countdown > 0) {\n",
    "      total = total * countdown\n",
    "      countdown = countdown - 1\n",
    "   }\n",
    "   return total\n",
    "}\n",
    "factorial(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factorialRec: (n: Int)Int\n",
       "res40: Int = 24\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorialRec(n: Int): Int = {\n",
    "   if(n == 0){\n",
    "     1\n",
    "   } else {\n",
    "     factorialRec(n - 1) * n\n",
    "   }\n",
    "}\n",
    "factorialRec(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "innerFactorial: (n: Int, total: Int)Int\n",
       "factorialTailRec: (n: Int)Int\n",
       "res75: Int = 24\n"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def innerFactorial(n: Int, total: Int): Int = {\n",
    "    if(n == 0) {\n",
    "        total\n",
    "    } else {\n",
    "        innerFactorial(n - 1, total * n)\n",
    "    }\n",
    "}\n",
    "def factorialTailRec(n: Int): Int = {\n",
    "    innerFactorial(n, 1)\n",
    "}\n",
    "factorialTailRec(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factorialFold: (n: Int)Int\n",
       "res8: Int = 24\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorialFold(n: Int): Int = (2 to n).fold(1)((acc, i) => acc * i)\n",
    "factorialFold(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 attending Denver all things Data\n",
      "There are 2 attending Denver all things Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Meetup\n",
       "defined class Person\n",
       "meetup: Meetup = Meetup@4f30a9d7\n",
       "meetupWithAttendees: Meetup = Meetup@12bf569c\n"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Meetup(val name:String, val attendeeList: List[Person] = List()) {\n",
    "   val memberVariable = \"I am scoped to the instance of this class\"\n",
    "   def addAttendee(person: Person) = new Meetup(name, person :: attendeeList)\n",
    "   def greet() = println(s\"The meetup called $name greets everyone!\")\n",
    "   def report() = println(s\"There are ${attendeeList.size} attending $name\")\n",
    "}\n",
    "case class Person(name: String)\n",
    "val meetup = new Meetup(\"Denver all things Data\")\n",
    "val meetupWithAttendees = meetup.addAttendee(Person(\"Christopher\"))\n",
    "                                .addAttendee(Person(\"Kara\"))\n",
    "\n",
    "meetup.report\n",
    "meetupWithAttendees.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Fraction\n",
       "f: Fraction = 1/2\n",
       "f2: Fraction = 1/4\n",
       "res60: Fraction = 6/8\n"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Fraction(val numerator: Int, val denominator: Int) {\n",
    "    def + (other: Fraction): Fraction = {\n",
    "        if(denominator == other.denominator){\n",
    "            new Fraction(numerator + other.numerator, denominator)\n",
    "        } else {\n",
    "            new Fraction(numerator * other.denominator + other.numerator * denominator, other.denominator * denominator)\n",
    "        }        \n",
    "    }\n",
    "    \n",
    "    override def toString() = s\"$numerator/$denominator\"\n",
    "}\n",
    "\n",
    "val f = new Fraction(1, 2)\n",
    "val f2 = new Fraction(1, 4)\n",
    "f + f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: Int = 15\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1,2,3,4,5).sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hasUppercase: (str: String)Boolean\n",
       "res: Boolean = true\n",
       "res2: Boolean = false\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hasUppercase(str: String): Boolean = {\n",
    "    var hasUpper = false\n",
    "    for (index <- 0 until str.size){\n",
    "        if(str(index).isUpper) {\n",
    "            hasUpper = true\n",
    "        }\n",
    "    }\n",
    "    hasUpper\n",
    "}\n",
    "val res = hasUppercase(\"Hello world\")\n",
    "val res2 = hasUppercase(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "betterHasUppercase: (str: String)Boolean\n",
       "str: String = Hello world\n",
       "res: Boolean = true\n",
       "res2: Boolean = false\n"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def betterHasUppercase(str: String): Boolean = {\n",
    "    str.exists((letter) => {\n",
    "        letter.isUpper\n",
    "    })\n",
    "}\n",
    "val str = \"Hello world\"\n",
    "println(str(0))\n",
    "val res = betterHasUppercase(\"Hello world\")\n",
    "val res2 = betterHasUppercase(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "listPlusOne: List[Int] = List(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
       "filteredList: List[Int] = List(6, 7, 8, 9, 10)\n",
       "flatList: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 6, 7, 8, 9, 10, 7, 8, 9, 10, 8, 9, 10, 9, 10, 10)\n",
       "uniq: scala.collection.immutable.Set[Int] = Set(5, 10, 1, 6, 9, 2, 7, 3, 8, 4)\n",
       "sum: Int = 55\n",
       "uniqueList: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "total: Int = 55\n",
       "res77: List[Unit] = List((), (), (), (), (), (), (), (), (), ())\n"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val list = List(1,2,3,4,5,6,7,8,9,10)\n",
    "val listPlusOne = list.map(i => i + 1)\n",
    "val filteredList = list.filter(i => i > 5)\n",
    "//val listOfLists = list.map(i => (i to 10))\n",
    "val flatList = list.flatMap(i => (i to 10))\n",
    "val uniq = flatList.toSet\n",
    "val sum = list.foldRight(0)((item, acc) => item + acc)\n",
    "val uniqueList = flatList.foldRight(List.empty[Int])((item, newList) => {\n",
    "    if(newList.contains(item)){ \n",
    "        newList \n",
    "    } else { \n",
    "        item :: newList\n",
    "    }\n",
    "})\n",
    "var total = 0\n",
    "list.map(x => total = total+ x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "areAllValuesGreaterThan0: Boolean = true\n",
       "areAllValuesGreaterThan10: Boolean = false\n",
       "doesThereExistAValuelessThan0: Boolean = false\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val areAllValuesGreaterThan0 = list.forall(_ > 0)\n",
    "val areAllValuesGreaterThan10 = list.forall(_ > 10)\n",
    "val doesThereExistAValuelessThan0 = list.exists(_ < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@47992e8c\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileRDD: org.apache.spark.sql.Dataset[String] = [value: string]\n",
       "r: Long = 495\n"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileRDD = spark.read.textFile(\"README.md\")\n",
    "val r = fileRDD.flatMap(_.split(\" \"))\n",
    "               .filter(!_.isEmpty)\n",
    "               .map(word => word.toUpperCase)\n",
    "               //.groupByKey(x => s\"${x(0)}\")\n",
    "               .count()\n",
    "//r.collect().sortBy(- _._2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "Spark is a fast and general cluster computing system for Big Data. It provides\n",
      "rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n",
      "and Spark Streaming for stream processing.\n",
      "<http://spark.apache.org/>\n",
      "You can find the latest Spark documentation, including a programming\n",
      "guide, on the [project web page](http://spark.apache.org/documentation.html).\n",
      "## Building Spark\n",
      "Spark is built using [Apache Maven](http://maven.apache.org/).\n",
      "To build Spark and its example programs, run:\n",
      "You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).\n",
      "[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n",
      "For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).\n",
      "The easiest way to start using Spark is through the Scala shell:\n",
      "    ./bin/spark-shell\n",
      "    ./bin/pyspark\n",
      "Spark also comes with several sample programs in the `examples` directory.\n",
      "    ./bin/run-example SparkPi\n",
      "examples to a cluster. This can be a mesos:// or spark:// URL,\n",
      "    MASTER=spark://host:7077 ./bin/run-example SparkPi\n",
      "Testing first requires [building Spark](#building-spark). Once Spark is built, tests\n",
      "[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).\n",
      "Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\n",
      "Hadoop, you must build Spark against the same version that your cluster runs.\n",
      "[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\n",
      "Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)\n",
      "in the online documentation for an overview on how to configure Spark.\n",
      "Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filteredSpark: org.apache.spark.sql.Dataset[String] = [value: string]\n",
       "foldCount: Int = 28\n",
       "sparkCount: Long = 28\n"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filteredSpark = fileRDD.filter(_.toUpperCase.contains(\"SPARK\"))\n",
    "val foldCount = filteredSpark.map(_ => 1).reduce(_ + _)\n",
    "val sparkCount = filteredSpark.count()\n",
    "filteredSpark.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
